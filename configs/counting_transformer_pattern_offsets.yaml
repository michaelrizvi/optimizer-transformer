output:
  folder: output/counting/transformer_pattern_offsets
  target_model_count: 1
dataset:
  name: counting
  counting:
    min_range_size: 1
    max_range_size: 5 
    vocab_size: 110 
    sep_token: 102
    pad_token: 103
model:
  arch: transformer
  transformer:
    d_model: 32 
    n_layers: 2
    n_heads: 4
    d_ff: 64 
    max_len: 128  # Increased from 64 to accommodate position offsets
    dropout: 0.0
  model_count_times_batch_size: 10000 
  init: regular
optimizer:
  name: PatternSearchFast
  epochs: 10000
  es_acc: 0.98
  use_position_offsets: true      # Enable positional offset training
  max_position_offset: 64         # Maximum offset to sample (sequences can start at positions 0-64)
distributed:
  loss_thres: "-999,999"
  num_samples: "2000"
  target_model_count_subrun: 1